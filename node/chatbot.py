import google.generativeai as genai
from openai import OpenAI
from nodes import NODE_CLASS_MAPPINGS as ALL_NODE_CLASS_MAPPINGS
from torchvision.transforms import ToPILImage
import torch
from PIL import Image
import numpy as np


def tensor2pil(tensor: torch.Tensor) -> Image.Image:
    if tensor.ndim == 4:
        tensor = tensor.squeeze(0)
    if tensor.ndim == 3 and tensor.shape[-1] == 3:
        np_image = (tensor.numpy() * 255).astype(np.uint8)
    else:
        raise ValueError(
            "Tensor ph·∫£i c√≥ shape [H, W, C] ho·∫∑c [1, H, W, C] v·ªõi C = 3 (RGB).")
    pil_image = Image.fromarray(np_image)
    return pil_image


class API_chatbot:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "chatbot": (["Gemini | 1.5 Flash", "Gemini | 1.5 Pro", "OpenAI | GPT 4-o mini", "OpenAI | GPT 4-o", "OpenAI | GPT 3.5 Turbo", "HuggingFace | Meta Llama-3.2"],),
                "preset": (["None", "Python Function"],),
                "APIkey": ("STRING", {"default": "", "multiline": False, "tooltip": "Chatbot API"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "tooltip": "The random seed"}),
                "main_prompt": ("STRING", {"default": "", "multiline": True, "tooltip": "Chatbot prompt"}),
                "sub_prompt": ("STRING", {"default": "", "multiline": True, "tooltip": "Chatbot prompt"})
            },
            "optional": {
                "image": ("IMAGE", {"tooltip": "The for gemini model"})
            }
        }

    CATEGORY = "‚ú® SDVN/API"

    RETURN_TYPES = ("STRING",)
    FUNCTION = "api_chatbot"

    def api_chatbot(self, chatbot, preset, APIkey, seed, main_prompt, sub_prompt, image=None):
        model_list = {
            "Gemini | 1.5 Flash": "gemini-1.5-flash",
            "Gemini | 1.5 Pro": "gemini-1.5-pro",
            "OpenAI | GPT 4-o mini": "gpt-4o-mini",
            "OpenAI | GPT 4-o": "gpt-4o",
            "OpenAI | GPT 3.5 Turbo": "gpt-3.5-turbo-0125",
            "HuggingFace | Meta Llama-3.2": "meta-llama/Llama-3.2-3B-Instruct"
        }
        preset_prompt = {
            "None": [],
            "Python Function": [
                {"role": "user", "content": "T√¥i s·∫Ω y√™u c·∫ßu m·ªôt h√†m def python v·ªõi nhi·ªám v·ª• b·∫•t k·ª≥, h√£y cho t√¥i c√¢u tr·∫£ l·ªùi l√† h√†m python ƒë√≥,vi·∫øt th·∫≠t ƒë∆°n gi·∫£n, v√† kh√¥ng c·∫ßn b·∫•t k·ª≥ h∆∞·ªõng d·∫´n n√†o kh√°c, c√°c import ƒë·∫∑t trong h√†m. ƒê·ªëi v·ªõi y√™u c·∫ßu ƒë·∫ßu v√†o ho·∫∑c ƒë·∫ßu ra l√† h√¨nh ·∫£nh, h√£y nh·ªõ ·∫£nh ·ªü d·∫°ng tensor"},
                {"role": "assistant", "content": "ƒê·ªìng √Ω! H√£y ƒë∆∞a ra y√™u c·∫ßu c·ªßa b·∫°n."}
            ]
        }
        if "DPRandomGenerator" in ALL_NODE_CLASS_MAPPINGS:
            cls = ALL_NODE_CLASS_MAPPINGS["DPRandomGenerator"]
            main_prompt = cls().get_prompt(main_prompt, seed, 'No')[0]
            sub_prompt = cls().get_prompt(sub_prompt, seed, 'No')[0]
        prompt = f"{main_prompt}.{sub_prompt}"
        model_name = model_list[chatbot]
        if 'Gemini' in chatbot:
            genai.configure(api_key=APIkey)
            model = genai.GenerativeModel(model_name)
            prompt += preset_prompt[preset][0]["content"] if preset != "None" else ""
            if image == None:
                response = model.generate_content(prompt)
            else:
                image = tensor2pil(image)
                response = model.generate_content([prompt, image])
            answer = response.text
        if "HuggingFace" in chatbot:
            answer = ""
            client = OpenAI(
                base_url="https://api-inference.huggingface.co/v1/", api_key=APIkey)
            messages = [
                {"role": "user", "content": prompt}
            ]
            messages = preset_prompt[preset] + messages
            stream = client.chat.completions.create(
                model=model_name,
                messages=messages,
                stream=True
            )
            for chunk in stream:
                answer += chunk.choices[0].delta.content
        if "OpenAI" in chatbot:
            answer = ""
            client = OpenAI(
                api_key=APIkey)
            messages = [
                {"role": "user", "content": prompt}
            ]
            messages = preset_prompt[preset] + messages
            stream = client.chat.completions.create(
                model=model_name,
                messages=messages,
                stream=True
            )
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    answer += chunk.choices[0].delta.content
        return (answer.strip(),)


NODE_CLASS_MAPPINGS = {
    "SDVN API chatbot": API_chatbot,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "SDVN API chatbot": "üí¨ API chatbot"
}
